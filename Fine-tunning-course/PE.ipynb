{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4159653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "\n",
    "# Set the path before importing HF libraries\n",
    "os.environ[\"HF_HOME\"] = \"/media/aiseed/AISeed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08ef7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "GPU memory: 12.4GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU - you will need to use a GPU to train models\")\n",
    "\n",
    "# Authenticate with Hugging Face (optional, for private models)\n",
    "from huggingface_hub import login\n",
    "# login()  # Uncomment if you need to access private models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b055e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069af81c595a4840831e6ffabcd1e432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb805d51d96d4d6f80d2a0b62d0c3335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc3dfe9d39d43c19f5a45f4b5df8dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3ab47b6405493889e603ae8d774472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5072f267ee5e488d842fe97e14e90f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fa34f3bf404d7aab916924241e4f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/182 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load both base and instruct models for comparison\n",
    "base_model_name = \"HuggingFaceTB/SmolLM3-3B-Base\"\n",
    "instruct_model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# Load tokenizers\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "\n",
    "# Load models (use smaller precision for memory efficiency)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52147d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model comparison ===\n",
      "\n",
      "ðŸ¤– BASE MODEL RESPONSE:\n",
      " What is the difference between classical computing and quantum computing?\n",
      "Quantum computing is a relatively new and emerging field of study that involves using the principles of quantum mechanics to perform computations. Quantum mechanics is the branch of physics that deals with the behavior of matter and energy at the smallest scales. At these scales, particles such as electrons and photons can exist in multiple states simultaneously, and their properties can be entangled with each other.\n",
      "Classical computing, on the other hand, is the traditional way of performing computations using bits, which can only be in one of two states: 0 or 1. Quantum computers use quantum bits or qubits, which can exist in multiple states simultaneously, making them much more powerful than classical computers for certain types of computations.\n",
      "\n",
      "\n",
      "==================================================\n",
      "Instruct model response:\n",
      "nowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "Explain quantum computing in simple terms.\n",
      "assistant\n",
      "<think>\n",
      "Okay, so I need to explain quantum computing in simple terms. Hmm, where do I start? I remember that quantum computing is different from classical computers, but I'm not exactly sure how. Let me think... Classical computers use bits, right? They can be either 0 or 1. But quantum computers use something called qubits. Wait, qubits can be in multiple states at once? Like, they can be both 0 and 1 at the same time? That sounds like superposition. So, if you have multiple qubits, the number of possible states grows exponentially. That's a big difference from classical bits, which just add up linearly. \n",
      "\n",
      "Then there's entanglement. I think that's\n"
     ]
    }
   ],
   "source": [
    "# Test the same prompt on both models\n",
    "test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "# Prepare the prompt for base model (no chat template)\n",
    "base_inputs = base_tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Prepare the prompt for instruct model (with chat template)\n",
    "instruct_messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "instruct_formatted = instruct_tokenizer.apply_chat_template(\n",
    "    instruct_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "instruct_inputs = instruct_tokenizer(instruct_formatted, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate responses\n",
    "print(\"=== Model comparison ===\\n\")\n",
    "\n",
    "print(\"ðŸ¤– BASE MODEL RESPONSE:\")\n",
    "with torch.no_grad():\n",
    "    base_outputs = base_model.generate(\n",
    "        **base_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=base_tokenizer.eos_token_id\n",
    "    )\n",
    "    base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    print(base_response[len(test_prompt):])  # Show only the generated part\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Instruct model response:\")\n",
    "with torch.no_grad():\n",
    "    instruct_outputs = instruct_model.generate(\n",
    "        **instruct_inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=instruct_tokenizer.eos_token_id\n",
    "    )\n",
    "    instruct_response = instruct_tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    assistant_start = instruct_response.find(\"<|im_start|>assistant\\n\") + len(\"<|im_start|>assistant\\n\")\n",
    "    assistant_response = instruct_response[assistant_start:].split(\"<|im_end|>\")[0]\n",
    "    print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27368e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING REASONING CAPABILITIES ===\n",
      "\n",
      "Problem 1: What is 15 Ã— 24? Show your work.\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "What is 15 Ã— 24? Show your work.\n",
      "assistant\n",
      "<think>\n",
      "Okay, so I need to figure out what 15 multiplied by 24 is. Let me think about how to approach this. I remember that multiplication can be done in a few different ways, like using the standard algorithm, breaking it down into smaller parts, or maybe even using some mental math tricks. Let me try the standard algorithm first because that's what I'm most familiar with.\n",
      "\n",
      "Alright, so 15 times 24. Let me write it down like I would on paper. I'll set it up as:\n",
      "\n",
      "â€ƒâ€ƒ15\n",
      "Ã— 24\n",
      "-------\n",
      "\n",
      "Now, I need to multiply 15 by each digit of 24, starting from the rightmost digit, which is the 4, and then the next digit, which is the 2. \n",
      "\n",
      "First, multiply 15 by 4. Let me do that. 15 times 4. Hmm, 10 times 4 is 40, and 5 times 4 is 20. So \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Problem 2: A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\n",
      "assistant\n",
      "<think>\n",
      "Okay, so I need to figure out how much flour is needed for 30 cookies if the recipe calls for 2 cups for 12 cookies. Hmm, let's see. First, maybe I should determine how much flour is used per cookie. If 2 cups are for 12 cookies, then I can divide 2 cups by 12 to find the amount per cookie. Let me write that down: 2 cups Ã· 12 cookies =?\n",
      "\n",
      "Calculating that, 2 divided by 12 is the same as 1/6. So each cookie requires 1/6 of a cup of flour. Now, if I want to make 30 cookies, I need to multiply the amount per cookie by 30. So, 1/6 cup per cookie times 30 cookies. Let me do that multiplication: (1/6) * 30.\n",
      "\n",
      "Hmm, 30 divided by 6 is 5. So 1/6 times 30\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Problem 3: If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\n",
      "assistant\n",
      "<think>\n",
      "Okay, let's see. The problem is asking how much money I have left after spending some amounts on lunch and a book. I start with $50. First, I need to figure out how much I spent in total. \n",
      "\n",
      "So, the lunch cost $18.75 and the book cost $12.30. To find the total spent, I should add those two amounts together. Let me do that step by step. \n",
      "\n",
      "Starting with $18.75 for lunch. If I add $12.30 to that, I can think of it as adding the dollars first and then the cents. The dollars are 18 + 12, which is 30. Then the cents are 75 + 30, which is 105 cents. But since cents go up to 99, 105 cents is the same as 1 dollar and 5 cents. So adding that to the 30 dollars gives me 31 dollars and 5 cents. \n",
      "\n",
      "Wait, let\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test SmolLM3's reasoning capabilities\n",
    "reasoning_prompts = [\n",
    "    \"What is 15 Ã— 24? Show your work.\",\n",
    "    \"A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\",\n",
    "    \"If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING REASONING CAPABILITIES ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(reasoning_prompts, 1):\n",
    "    print(f\"Problem {i}: {prompt}\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = instruct_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = instruct_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.3,  # Lower temperature for more consistent reasoning\n",
    "            do_sample=True,\n",
    "            pad_token_id=instruct_tokenizer.eos_token_id\n",
    "        )\n",
    "        response = instruct_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_start = response.find(\"<|im_start|>assistant\\n\") + len(\"<|im_start|>assistant\\n\")\n",
    "        assistant_response = response[assistant_start:].split(\"<|im_end|>\")[0]\n",
    "        print(f\"Answer: {assistant_response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5ea60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09208b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING GSM8K DATASET ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04048d7ede8043df8b5aaf4088c63c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78f9fd683d04a97acfd1793720174f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff097f84a55b4b54ab5cf38a32ca9bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903e1ead73574090aaf910e016ded5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608ccde83d0d4bb99ab8e653494d3cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GSM8K example: {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3910837b18d4a6ca8cdc7085fff9153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed example: {'messages': [{'content': 'You are a math tutor. Solve problems step by step.', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}, {'content': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Function to process different dataset formats\n",
    "def process_qa_dataset(examples, question_col, answer_col):\n",
    "    \"\"\"Process Q&A datasets into chat format\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for question, answer in zip(examples[question_col], examples[answer_col]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    \n",
    "    return {\"messages\": processed}\n",
    "\n",
    "def process_instruction_dataset(examples):\n",
    "    \"\"\"Process instruction-following datasets\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    \n",
    "    return {\"messages\": processed}\n",
    "\n",
    "# Example: Process GSM8K math dataset\n",
    "print(\"=== PROCESSING GSM8K DATASET ===\\n\")\n",
    "\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:100]\")  # Small subset for demo\n",
    "print(f\"Original GSM8K example: {gsm8k[0]}\")\n",
    "\n",
    "# Convert to chat format\n",
    "def process_gsm8k(examples):\n",
    "    processed = []\n",
    "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a math tutor. Solve problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    return {\"messages\": processed}\n",
    "\n",
    "gsm8k_processed = gsm8k.map(process_gsm8k, batched=True, remove_columns=gsm8k.column_names)\n",
    "print(f\"Processed example: {gsm8k_processed[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab52fdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0762c061b8d40889b48ed7d4ee57fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORMATTED TRAINING DATA ===\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a math tutor. Solve problems step by step.\n",
      "\n",
      "<|im_start|>user\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to apply chat templates to processed datasets\n",
    "def apply_chat_template_to_dataset(dataset, tokenizer):\n",
    "    \"\"\"Apply chat template to dataset for training\"\"\"\n",
    "    \n",
    "    def format_messages(examples):\n",
    "        formatted_texts = []\n",
    "        \n",
    "        for messages in examples[\"messages\"]:\n",
    "            # Apply chat template\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False  # We want the complete conversation\n",
    "            )\n",
    "            formatted_texts.append(formatted_text)\n",
    "        \n",
    "        return {\"text\": formatted_texts}\n",
    "    \n",
    "    return dataset.map(format_messages, batched=True)\n",
    "\n",
    "# Apply to our processed GSM8K dataset\n",
    "gsm8k_formatted = apply_chat_template_to_dataset(gsm8k_processed, instruct_tokenizer)\n",
    "print(\"=== FORMATTED TRAINING DATA ===\")\n",
    "print(gsm8k_formatted[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292d9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFaceTB/SmolLM3-3B-Base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6ce520f3c44e2daf920890358741a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Parameters: 3,075,098,624\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for fine-tuning\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# import wandb  # Optional: for experiment tracking\n",
    "\n",
    "# Initialize Weights & Biases (optional)\n",
    "# wandb.init(project=\"smollm3-finetuning\")\n",
    "\n",
    "# Load SmolLM3 base model for fine-tuning\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B-Base\"\n",
    "new_model_name = \"SmolLM3-Custom-SFT\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "tokenizer.padding_side = \"right\"  # Padding on the right for generation\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8adb277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATASET ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5199d896f8ec4c059076e2822d41a834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f61c59b6ea4a78a14ffef9c2c39baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d887882b8d4571b54099be1fcf7b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5db83ad62264e96a0876655063eedd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791bf7b5e08c401d9b56beb59b2dfb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/LongAlign_64k_Qwen3_32B_yarn_131k_th(â€¦):   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86a40d3a57e45cfb24bd7029bdbdb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/LongAlign_64k_Qwen3_32B_yarn_131k_th(â€¦):   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6f1ebffe741efae531ce3b2ffb0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/113 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014fcc16a2614587b189969a16b44440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00000-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55394e070ca49df98fb503b2477a77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00001-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089c892c3bcf4ebe97afafba3a328bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00002-of-00(â€¦):   0%|          | 0.00/288M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc747e6c9bd4946b0b1250c8cb20ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00003-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5630b1e8030944ed966c162b155c0610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00004-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3746f8b339634c3c84094b376921955c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00005-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2b023c2df7421eba6dbfe938948696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00006-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408fd0a554e04e20b1003208854209c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00007-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7947110cad40e9aeeaa9c1b2968929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00008-of-00(â€¦):   0%|          | 0.00/288M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a3032c97fe466490051cc3af2f3c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00009-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8c4df98d3b4f839b254fc487f5cad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00010-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadadca567cc494a9c0e0ebc067701d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00011-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500281a539584212b187494aec8755d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00012-of-00(â€¦):   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c9c9286b3407aa02ef0de5b7864c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00013-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d81167aba542f291254f685b4b8d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00014-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dddf8cc3094a6cbdcfd1398ce83940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00015-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7391b2e16a7145799dcefb903448ceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00016-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6405d166614345ac18d9c1d7a50559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00017-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7866eefad69c45efac610f1812c7aece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00018-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b395c378444f4caae45017b71fcc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00019-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ce71b6340042f3804c514f20560296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00020-of-00(â€¦):   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa3127a56e349d8b1312f0ef99440c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00021-of-00(â€¦):   0%|          | 0.00/288M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e245784a5b04998ab9cc44fa90a616b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00022-of-00(â€¦):   0%|          | 0.00/282M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e1c9ce0cbc460094314c057225cf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00023-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcbb2a421e7467289680a83a49ab221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00024-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4f7fc3522843a8b20ca15d1090f662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00025-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5878f4dc18674b3c999655682335d51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00026-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdd05f6c29e47a4a361e85f8b9c74a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00027-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3becb58a9c4c0d9bcd94e08156ec47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00028-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bfc1149aab455dbd81c4fa6d10a3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00029-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534b6aa9cbe4471e95075bff3d559fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00030-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef959a3f796c48da9db7eef35ea319bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00031-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c36087c248486099733b36912a7bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00032-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092ecba7996a4e588ed31402a181b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00033-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f589ea86c674501818f3270be5d26fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00034-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aea3f2a15e48159e9e13e67101099c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00035-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2a5603b4344b9ab901f52f8c34cbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00036-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b18afd784748c6a8cefa6ff06931f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00037-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db42f04ceb214e1b9acb3149b3b4e555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00038-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bac1eef341c4003b5296d8b2afb892f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00039-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3051023f5e2540099dbdc9f489b51abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00040-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c74b1aa3acf4d6e84e0f18492421af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00041-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1de3e706224cccba0a00b8dbff0652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00042-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bf8554031d4ae9a7c205c41dddcae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00043-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81866746a374940852072658bc46b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00044-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b60e974b234ce199f1f022e83e2381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00045-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171bf0acbfd348a6b10e8934eea87333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00046-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31784bcd7414a55b8c4880d672260e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00047-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d797f56686141d7ab64508666ed8346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00048-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0acef07c444c848284a7e51b195609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00049-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6979bfa9bc494e44a6fbfaab9db6363b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00050-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13c2ded22814e5da1d84e882c83db06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00051-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105f565d01d3442ba8564c4130bd996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00052-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599c999358e74786bea64725641e637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00053-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8158fafbe51442f9bd5a64b0619fe9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00054-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da5c1f7e17e42fe8659137fa16a2163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00055-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c28bc468e44d6a8762b04073f004af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00056-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5336c144024a53b0e7c735159a6e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00057-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4607076054514e699075e3ba346f64ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00058-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ae2a9bed0496ca80281fa0c033d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00059-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4a6cdc0e1e4f9097acb89e82a3ed8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00060-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6576946c5741b7b97e3f4cc7ee2ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00061-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd0d6fa3ee4ba59de765fe21a651c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00062-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28abf901eae49639435c95d31968d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00063-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f7b55b51a041a5bb95f40ac00c96cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00064-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99da492b5304ea692f1c228331984d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00065-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c623950d0b844f1b988c4e3c991084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00066-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605897c7584f4b348a043fa25bc2ad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00067-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e694135396d94281865df6f70e53be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00068-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bef37dac5b4c4886cc1e1e5f1b8ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00069-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723d94b0209548a7a7320ff7a569a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00070-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0327782c3f4fb492503f31682fc87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00071-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5968cd2a67042e991fa700ec04f5022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00072-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11cc8418a554ec7b59583687fd44ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00073-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcfdf13d6ea448e87474d830a180adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00074-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90214b9577ba4adbb7b778b990249126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00075-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bf620882c749d58f32a4f4f189c297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00076-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7de3edb525f40b3b24ae7e370783184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00077-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ad3e986c574febb7970720cda3d4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00078-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99388b6f353747a5a88bab91c9ba1642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00079-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee74a54b15424f8f9e1de5533a6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00080-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e5fa460a1145c3913d6f368fa3517e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00081-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1003239fde164aba83ab1090374d951a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00082-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b3ac54834e4c2f9d2d559c2a09d8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00083-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2615007a2c54fc0b98566e49410982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00084-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237fd6d4a06b47488f333208e5f7fc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00085-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea386a856e449288ce2e3d132808064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00086-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21df898c1abf48cb80d3009ca6735b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00087-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76a076defad4f66b97653fbc4e26209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00088-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac16f86764548849d56e91365882a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00089-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bceba3d5748b4734ad6c7e7419355ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00090-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450a7d4ce4fa48ed9fda5a20197db755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00091-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f1030f2c904f8bbe6ba9683caf1052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00092-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107cff58b12b49429997563a72269b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00093-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8384b8bc084f37812188f241e41368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00094-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4676fb0691344079bb9deae00ea76611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00095-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4456d9ff5ac44a8d9bbf29a5f1a2202f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00096-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9b7e176bb4eacb7046a32af4af369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00097-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08186e74eda4d90b9a1061af42b470e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00098-of-00(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1e238d5bb44ec93128b662fa314ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00099-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da56bdcbcb745f3812dfb6697db011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00100-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5b4d091ff448ccbf535166b4e92edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00101-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1494592dce4f7fae7bcb982ec1ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00102-of-00(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d17063de4b4a9298574f0160e0c328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00103-of-00(â€¦):   0%|          | 0.00/158M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d2c52386c4b1fb0892f02ccbf2054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00104-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea87ec84d784ce88c77ce3bbbcba600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00105-of-00(â€¦):   0%|          | 0.00/149M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598a91f2435a4bb1af511a61be9d98d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00106-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de25b6ce5fcd4010b783f678f9e6d2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00107-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b38b0a3f164182a898f61d64b897ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00108-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dacd8c0b344c698dd14e815bca5cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00109-of-00(â€¦):   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4a8c847d7e446190dc4adc3a80bd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00110-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eef2a0bae314811b084971583fd2d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00111-of-00(â€¦):   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488e8ef167654ca79e3887ce31b0d9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_think-00112-of-00(â€¦):   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fff552107b4e68b235402677b37d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/aya_dataset_Qwen3_32B_think-00000-of(â€¦):   0%|          | 0.00/32.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4688b6c85f74c4e9cd94725fd450fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/multi_turn_reasoning_if_think-00000-(â€¦):   0%|          | 0.00/178M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e31370e7f8447baf63e2ad15076ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/s1k_1.1_think-00000-of-00001.parquet:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa297ffa69945a1b762a27162a4926b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smolagents_toolcalling_traces_think-(â€¦):   0%|          | 0.00/81.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd9b05d0db04bd38a484dda13545105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_everyday_convs_reasoning_Qw(â€¦):   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef53487555f747e88ddd37e7c61b7959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual8_Qwen3_32B_thi(â€¦):   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ea26b28897433e8bff0376cbbc2dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual8_Qwen3_32B_thi(â€¦):   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d399f503e956418cb89a0a2868c78e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual8_Qwen3_32B_thi(â€¦):   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315665f23fcc4dde9c21795f6c8d0bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual8_Qwen3_32B_thi(â€¦):   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35fca09dee446deb28f0fcaab11cb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_systemchats_Qwen3_32B_think(â€¦):   0%|          | 0.00/64.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e1f5498944c0ab67fbbc577ac5c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/table_gpt_Qwen3_32B_think-00000-of-0(â€¦):   0%|          | 0.00/32.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea33a7435099428c9300800577cc8101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/LongAlign_64k_context_lang_annotated(â€¦):   0%|          | 0.00/199M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9345f4cb78c4487489cd2c374dfe4112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/Mixture_of_Thoughts_science_no_think(â€¦):   0%|          | 0.00/63.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dd90e03bf049189887b5617e13a998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenHermes_2.5_no_think-00000-of-000(â€¦):   0%|          | 0.00/164M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa65ccd0fdb44d08344d29943fcdb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenHermes_2.5_no_think-00001-of-000(â€¦):   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1912c6b7329e4ff1a4b9a518745ec539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_no_think_no_think(â€¦):   0%|          | 0.00/245M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d72c8aafd4f3fa5584784f133cca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_no_think_no_think(â€¦):   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099cc361cabf4f699212294687157dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/OpenThoughts3_1.2M_no_think_no_think(â€¦):   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ad1554ce248299f7dc79473b59cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/hermes_function_calling_v1_no_think-(â€¦):   0%|          | 0.00/10.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb74ad6fd6af4dd59e8e74be45b1e1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual_8languages_lan(â€¦):   0%|          | 0.00/158M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498a32e5881543dd8ab9b03f566e01c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_multilingual_8languages_lan(â€¦):   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73a3739b88644aea63122313379812a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_everyday_conversati(â€¦):   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f2471d1d614ed784ab4c50e072a5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_explore_instruct_re(â€¦):   0%|          | 0.00/5.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1171580cb8c34bffa01d947b0b17cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c4e13dea0462fb175ac7c7c813d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab4328e94d4229bfc5ae6b9ff37105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d56bc8a08e47afa16ee317b0382c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9194484d52b94894a915e9894de4b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d8b7ab32c749808828f17cee52a88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_magpie_ultra_n(â€¦):   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b322581a84a946c18d586fb77d64e5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_rewrite_no_thi(â€¦):   0%|          | 0.00/38.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b8e9b4f4f64f4d857e036400cebc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_smol_summarize_no_t(â€¦):   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b8ed514bc647c69b87d16ade407daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/smoltalk_smollm3_systemchats_30k_no_(â€¦):   0%|          | 0.00/47.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d6279e910344c797944480b33797b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/table_gpt_no_think-00000-of-00001.pa(â€¦):   0%|          | 0.00/12.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f05baae26040a5a583f827c2828778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/tulu_3_sft_personas_instruction_foll(â€¦):   0%|          | 0.00/33.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a0fb52831d4dd8bd4d8bf576df5c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SFT/xlam_traces_no_think-00000-of-00001.(â€¦):   0%|          | 0.00/30.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e8dcacb8ba4d8193394f098299ca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating LongAlign_64k_Qwen3_32B_yarn_131k_think split:   0%|          | 0/7526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3e1bd7b7b84508a8234190584b6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating OpenThoughts3_1.2M_think split:   0%|          | 0/1133524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4607524aa2784db59f90e5dae336a976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating aya_dataset_Qwen3_32B_think split:   0%|          | 0/15222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee5c951ddfa414d88acded7dabf57f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating multi_turn_reasoning_if_think split:   0%|          | 0/28217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ffa00cfc71478b8319344274955ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating s1k_1.1_think split:   0%|          | 0/835 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcdfa858413472baaef6d1d854c00ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smolagents_toolcalling_traces_think split:   0%|          | 0/9079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef2a3134af74440b7e2a52872a69188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_everyday_convs_reasoning_Qwen3_32B_think split:   0%|          | 0/2057 [00:00<?, ? examplâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae9a021613b4d839f2ec9f5cb49b2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_multilingual8_Qwen3_32B_think split:   0%|          | 0/244736 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142aa43141f745c4a4d882c6b0439d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_systemchats_Qwen3_32B_think split:   0%|          | 0/27436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd77117626046efbfd6be7dcf3e35ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating table_gpt_Qwen3_32B_think split:   0%|          | 0/13201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb96c7483c48b78650f6568e7fe8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating LongAlign_64k_context_lang_annotated_lang_6_no_think split:   0%|          | 0/6249 [00:00<?, ? exaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443ce6aa73d5486386ba6152a1984662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Mixture_of_Thoughts_science_no_think split:   0%|          | 0/86110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744b9dc473254b8ea8351c6d0648bf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating OpenHermes_2.5_no_think split:   0%|          | 0/384900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed3cf9747f74a1bac610a72c842652b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating OpenThoughts3_1.2M_no_think_no_think split:   0%|          | 0/435193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d928461540d4e4887e357530968345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating hermes_function_calling_v1_no_think split:   0%|          | 0/8961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab203fa555464375909ef009c895210d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_multilingual_8languages_lang_5_no_think split:   0%|          | 0/254047 [00:00<?, ? exampâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e9ee40f7f34321a25a6825a0f8c4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_everyday_conversations_no_think split:   0%|          | 0/2260 [00:00<?, ? exampleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3b7946b6c04f6abf0cd789d4034188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_explore_instruct_rewriting_no_think split:   0%|          | 0/30391 [00:00<?, ? exâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d472a72b74f54f9ea65fbebde010a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_smol_magpie_ultra_no_think split:   0%|          | 0/406843 [00:00<?, ? examples/sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c2f053e9b74a7fbe25447396e56ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_smol_rewrite_no_think split:   0%|          | 0/53262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb14e222ee4aed8538ace8f5787953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_smol_summarize_no_think split:   0%|          | 0/96061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0726645a8942410bad0e45c904ec5a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating smoltalk_smollm3_systemchats_30k_no_think split:   0%|          | 0/33997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0ef2c787db4cf7919f2c2ff952a05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating table_gpt_no_think split:   0%|          | 0/13203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b18bdf3658402db253056f3559e8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating tulu_3_sft_personas_instruction_following_no_think split:   0%|          | 0/29970 [00:00<?, ? examâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cdfe00f0f746b6b742c98f9476b3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating xlam_traces_no_think split:   0%|          | 0/59962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea2f55c6734c04a04c0040cae3b40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1000\n",
      "Example: {'messages': [{'content': 'Hi there', 'role': 'user'}, {'content': '<think>\\nOkay, the user sent \"Hi there\". That\\'s a friendly greeting. I should respond in a welcoming way. Let me check the guidelines. I need to be helpful, keep the conversation going, and maybe ask how I can assist them. Let me make sure the tone is warm and approachable. Alright, something like \"Hello! How can I assist you today?\" That should work. Let me confirm there\\'s no typo and it\\'s in a natural, conversational style.\\n</think>\\n\\nHello! How can I assist you today?', 'role': 'assistant'}, {'content': \"I'm looking for a healthy breakfast idea. What's a good option?\", 'role': 'user'}, {'content': \"<think>\\nOkay, the user is asking for a healthy breakfast idea. Let me think about what makes a breakfast healthy. It should be balanced, providing a mix of nutrients like protein, fiber, healthy fats, and some carbs. Let me brainstorm some options.\\n\\nMaybe start with a classic like avocado toast. It's popular and has good fats and fiber. Adding an egg or Greek yogurt could boost the protein. Oh, and maybe some veggies on top for extra nutrients. That could work.\\n\\nAnother option could be a smoothie. Smoothies are quick and can pack a lot of nutrients. Using spinach, banana, berries, and a protein powder or Greek yogurt would be good. Maybe add some nut butter for healthy fats. That's a solid choice too.\\n\\nOatmeal is another staple. Overnight oats are convenient. Mixing oats with milk, chia seeds, and fruits. Maybe add some nuts or seeds for crunch. It's high in fiber and can be customized.\\n\\nWhat about a breakfast bowl with quinoa or another grain? Topped with fruits, nuts, and a drizzle of honey or nut butter. That's another balanced option.\\n\\nI should also consider dietary restrictions. Maybe mention alternatives for those who are vegan, gluten-free, etc. For example, using almond milk instead of dairy in the smoothie or choosing gluten-free oats.\\n\\nPortion sizes and preparation time might be important too. The user might be looking for something quick or something they can prep ahead. The avocado toast is quick, the overnight oats can be prepped the night before.\\n\\nIncluding a variety of options gives the user choices based on their preferences and time. Let me structure the response with a couple of options, each with ingredients and preparation steps. Also, highlight the nutritional benefits of each. Make sure to keep the tone friendly and helpful.\\n</think>\\n\\nHereâ€™s a quick and nutritious breakfast idea thatâ€™s easy to customize:\\n\\n### **Avocado Toast with a Twist**  \\n**Ingredients:**  \\n- 1 slice whole-grain or sourdough bread (toasted)  \\n- Â½ ripe avocado, mashed  \\n- 1 poached or scrambled egg (or a dollop of Greek yogurt for a vegan option)  \\n- Sprinkle of chili flakes, black pepper, and a pinch of sea salt  \\n- Optional toppings: cherry tomatoes, microgreens, or a drizzle of olive oil  \\n\\n**Why itâ€™s healthy:**  \\n- **Whole grains** provide fiber for sustained energy.  \\n- **Avocado** adds healthy fats and potassium.  \\n- **Eggs/Greek yogurt** offer high-quality protein to keep you full.  \\n- **Spices** add flavor without extra calories.  \\n\\n---\\n\\n### **Protein-Packed Smoothie Bowl**  \\n**Ingredients:**  \\n- 1 frozen banana  \\n- Â½ cup frozen berries (blueberries, strawberries, etc.)  \\n- Â½ cup unsweetened almond milk (or any milk of choice)  \\n- 1 scoop vanilla protein powder (or Â¼ cup Greek yogurt)  \\n- 1 tbsp chia seeds or flaxseeds  \\n- Toppings: sliced fruit, granola, nuts, or cacao nibs  \\n\\n**Blend it up**, pour into a bowl, and add toppings.  \\n\\n**Why itâ€™s healthy:**  \\n- **Fiber and antioxidants** from fruits and seeds.  \\n- **Protein** supports muscle repair and satiety.  \\n- **Healthy fats** from seeds or nut toppings.  \\n\\n---\\n\\n### **Overnight Oats (Make-Ahead Option)**  \\n**Ingredients:**  \\n- Â½ cup rolled oats  \\n- Â½ cup milk (dairy or plant-based)  \\n- 1 tbsp nut butter (peanut, almond, etc.)  \\n- 1 tsp honey or maple syrup  \\n- Â½ tsp chia seeds  \\n- Toppings: fresh fruit, cinnamon, or a sprinkle of granola  \\n\\n**Instructions:**  \\nMix all ingredients in a jar, refrigerate overnight, and enjoy cold or at room temperature.  \\n\\n**Why itâ€™s healthy:**  \\n- **Slow-digesting carbs** for steady energy.  \\n- **Chia seeds** add omega-3s and fiber.  \\n- **Nut butter** provides protein and healthy fats.  \\n\\n---\\n\\nLet me know if youâ€™d like more ideas or adjustments for dietary needs (e.g., vegan, gluten-free, low-carb)! ðŸ˜Š\", 'role': 'assistant'}], 'chat_template_kwargs': {'custom_instructions': '', 'enable_thinking': True, 'python_tools': [], 'xml_tools': []}, 'source': 'smoltalk-everyday-convs-reasoning-Qwen3-32B'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a686158af142f4bf4f422cd674b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted example: <|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 24 December 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hu...\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare training dataset\n",
    "print(\"=== PREPARING DATASET ===\\n\")\n",
    "\n",
    "# Option 1: Use SmolTalk2 (recommended for beginners)\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk2\", \"SFT\")\n",
    "train_dataset = dataset[\"smoltalk_everyday_convs_reasoning_Qwen3_32B_think\"].select(range(1000))  # Use subset for faster training\n",
    "\n",
    "# Option 2: Use your own processed dataset from Exercise 2\n",
    "# train_dataset = gsm8k_formatted.select(range(500))\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Example: {train_dataset[0]}\")\n",
    "\n",
    "# Prepare the dataset for SFT\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format the messages using the chat template\"\"\"\n",
    "    if \"messages\" in example:\n",
    "        # SmolTalk2 format\n",
    "        messages = example[\"messages\"]\n",
    "    else:\n",
    "        # Custom format - adapt as needed\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
    "        ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = train_dataset.map(format_chat_template)\n",
    "formatted_dataset = formatted_dataset.remove_columns(\n",
    "    [col for col in formatted_dataset.column_names if col != \"text\"]\n",
    ")\n",
    "print(f\"Formatted example: {formatted_dataset[0]['text'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
